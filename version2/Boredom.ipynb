{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Solely Off of Boredom Labels\n",
    "\n",
    "## Working with a multi-label dataset is too complicated as the data is very unbalanced. So for now, we will only pick 1 label from the dataset, I picked boredom because it is the least unbalanced out of the four labels.\n",
    "\n",
    "Create a new dataset by keeping only the 'Boredom' label column and balancing the classes by reducing all videos to match the size of the smallest class within the 'Boredom' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced boredom dataset class distribution:\n",
      "(8925, 2)\n",
      "Boredom\n",
      "0    3822\n",
      "1    2850\n",
      "2    1923\n",
      "3     330\n",
      "Name: count, dtype: int64\n",
      "-------------------------------------------s\n",
      "Balanced dataset class distribution:\n",
      "Boredom\n",
      "0    330\n",
      "1    330\n",
      "2    330\n",
      "3    330\n",
      "Name: count, dtype: int64\n",
      "-------------------------------------------\n",
      "{'110001': 10, '110005': 10, '110006': 10, '110007': 10, '110012': 10, '110014': 10, '110015': 10, '110017': 10, '111003': 10, '181374': 10, '200050': 10, '202614': 10, '205601': 10, '210052': 10, '210053': 10, '210055': 10, '210057': 10, '210058': 10, '210059': 10, '210060': 10, '210061': 10, '226051': 10, '240846': 10, '310062': 10, '310070': 10, '310072': 10, '310074': 10, '310075': 10, '310076': 10, '310077': 10, '310078': 10, '310079': 10, '310082': 10, '334463': 10, '342227': 10, '350361': 10, '400018': 10, '400022': 10, '400030': 10, '400033': 10, '410019': 10, '410020': 10, '410024': 10, '410025': 10, '410026': 10, '410028': 10, '410029': 10, '410030': 10, '410032': 10, '411021': 10, '411031': 10, '414081': 10, '459999': 10, '500039': 10, '500044': 10, '500067': 10, '500095': 10, '510034': 10, '510035': 10, '510038': 10, '510040': 10, '510042': 10, '510046': 10, '522129': 10, '556463': 10, '567496': 10, '799402': 10, '826382': 10, '826412': 10, '882654': 10}\n",
      "\n",
      "Length of the updated dataset:\n",
      "700\n",
      "Train set size: 489\n",
      "Validation set size: 140\n",
      "Test set size: 71\n",
      "Train, validation, and test splits saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "label_set = pd.read_csv(r\"C:\\Users\\ahmad\\Desktop\\EngagementML\\DAiSEE\\Labels\\AllLabels.csv\")\n",
    "\n",
    "new_label_set = label_set.drop(columns=['Engagement', 'Confusion', 'Frustration'], axis=1)\n",
    "\n",
    "print(\"Unbalanced boredom dataset class distribution:\")\n",
    "print(new_label_set.shape)\n",
    "print(new_label_set['Boredom'].value_counts())\n",
    "print('-------------------------------------------s')\n",
    "\n",
    "\n",
    "# Balance the dataset by sampling an equal number of videos for each class in 'Boredom'\n",
    "# The size is limited to the smallest class size\n",
    "min_class_size = 330\n",
    "balanced_data = new_label_set.groupby('Boredom', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_class_size, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Check the new balanced distribution\n",
    "print(\"Balanced dataset class distribution:\")\n",
    "print(balanced_data['Boredom'].value_counts())\n",
    "\n",
    "print('-------------------------------------------')\n",
    "\n",
    "# Further balance: Ensure all persons in the dataset have an equal number of videos (10 videos per person)\n",
    "# Extract 'person_id' from the 'ClipID' column\n",
    "balanced_data['person_id'] = balanced_data['ClipID'].str[:6]\n",
    "balanced_data = balanced_data.groupby('person_id', group_keys=False).apply(\n",
    "    lambda x: x.sample(10, random_state=42) if len(x) >= 10 else None\n",
    ").dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Check the updated counts\n",
    "person_counts = Counter(balanced_data['person_id'])\n",
    "print(dict(person_counts))\n",
    "\n",
    "print(\"\\nLength of the updated dataset:\")\n",
    "print(len(balanced_data))\n",
    "\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data into training and temp (for validation and testing)\n",
    "train_data, temp_data = train_test_split(\n",
    "    balanced_data, test_size=(1 - train_ratio), random_state=42, stratify=balanced_data['Boredom']\n",
    ")\n",
    "\n",
    "# Further split the temp data into validation and testing\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=(test_ratio / (test_ratio + val_ratio)), random_state=42, stratify=temp_data['Boredom']\n",
    ")\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(\"Train set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "# Save each split as a CSV file\n",
    "train_data.to_csv(\"train_labels.csv\", index=False)\n",
    "val_data.to_csv(\"val_labels.csv\", index=False)\n",
    "test_data.to_csv(\"test_labels.csv\", index=False)\n",
    "\n",
    "print(\"Train, validation, and test splits saved as CSV files.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To simplify training without a GPU, weâ€™ll use a smaller subset of the dataset. This reduces computational load, speeds up experimentation, and lets us validate the pipeline before scaling to the full dataset. To put it simply, we will create a new dataset out of the current one we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset class distribution:\n",
      "Boredom\n",
      "0    25\n",
      "1    25\n",
      "2    25\n",
      "3    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length of the reduced dataset:\n",
      "100\n",
      "{'500044': 2, '110006': 1, '210061': 3, '310072': 1, '500095': 1, '110001': 1, '310074': 4, '350361': 2, '110007': 2, '410020': 2, '510038': 1, '110015': 4, '410030': 3, '882654': 2, '310079': 2, '567496': 4, '500067': 3, '510046': 2, '310062': 2, '510035': 2, '210059': 1, '459999': 1, '310070': 1, '410026': 4, '210052': 1, '410032': 2, '202614': 3, '181374': 2, '310077': 1, '205601': 1, '210053': 2, '414081': 1, '410029': 2, '200050': 4, '334463': 1, '210058': 2, '826412': 1, '410024': 2, '410028': 2, '510040': 1, '110014': 1, '310082': 3, '110017': 1, '411021': 2, '310076': 1, '556463': 1, '110012': 1, '310075': 2, '400018': 1, '411031': 1, '410019': 1, '240846': 1, '210060': 1, '111003': 1, '210055': 1, '510034': 1, '410025': 1}\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# Target size per class\n",
    "videos_per_class = 25\n",
    "\n",
    "# Downsample each class\n",
    "small_dataset = balanced_data.groupby('Boredom', group_keys=False).apply(\n",
    "    lambda x: x.sample(videos_per_class, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Check the reduced dataset\n",
    "print(\"Reduced dataset class distribution:\")\n",
    "print(small_dataset['Boredom'].value_counts())\n",
    "\n",
    "print(\"\\nLength of the reduced dataset:\")\n",
    "print(len(small_dataset))\n",
    "\n",
    "person_counts = Counter(small_dataset['person_id'])\n",
    "print(dict(person_counts))\n",
    "print(len(person_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset class distribution:\n",
      "Boredom\n",
      "0    32\n",
      "1    32\n",
      "2    32\n",
      "3    32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length of the reduced dataset:\n",
      "128\n",
      "{'240846': 2, '210060': 3, '210059': 2, '310079': 3, '110012': 3, '210055': 2, '826382': 3, '882654': 2, '110017': 2, '310078': 3, '181374': 2, '210057': 3, '410020': 2, '510035': 3, '110001': 3, '400018': 3, '310072': 1, '510046': 3, '310076': 3, '500044': 3, '500095': 2, '334463': 3, '410028': 2, '342227': 3, '310074': 2, '110007': 3, '205601': 3, '210052': 3, '410026': 3, '110015': 3, '410032': 3, '410024': 3, '350361': 2, '556463': 3, '400033': 3, '410029': 2, '400030': 2, '111003': 2, '310075': 3, '210058': 3, '500039': 3, '110014': 3, '500067': 1, '411031': 3, '200050': 3, '826412': 2, '510034': 2, '410030': 3, '410019': 2, '411021': 2}\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Target size per person\n",
    "videos_per_person = 3\n",
    "\n",
    "# Downsample to ensure diversity: max 3 videos per person\n",
    "small_dataset = balanced_data.groupby('person_id', group_keys=False).apply(\n",
    "    lambda x: x.sample(videos_per_person, random_state=42) if len(x) >= videos_per_person else None\n",
    ").dropna().reset_index(drop=True)\n",
    "\n",
    "# Ensure class balance: Equal number of videos for each boredom level\n",
    "min_samples_per_class = small_dataset['Boredom'].value_counts().min()\n",
    "smalL_balanced_dataset = small_dataset.groupby('Boredom', group_keys=False).apply(\n",
    "    lambda x: x.sample(min_samples_per_class, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Check distribution\n",
    "print(\"Reduced dataset class distribution:\")\n",
    "print(smalL_balanced_dataset['Boredom'].value_counts())\n",
    "\n",
    "print(\"\\nLength of the reduced dataset:\")\n",
    "print(len(smalL_balanced_dataset))\n",
    "\n",
    "\n",
    "person_counts = Counter(smalL_balanced_dataset['person_id'])\n",
    "print(dict(person_counts))\n",
    "print(len(person_counts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 89\n",
      "Validation set size: 26\n",
      "Test set size: 13\n",
      "Train, validation, and test splits saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data into training and temp (for validation and testing)\n",
    "train_data, temp_data = train_test_split(\n",
    "    smalL_balanced_dataset, test_size=(1 - train_ratio), random_state=42, stratify=smalL_balanced_dataset['Boredom']\n",
    ")\n",
    "\n",
    "# Further split the temp data into validation and testing\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=(test_ratio / (test_ratio + val_ratio)), random_state=42, stratify=temp_data['Boredom']\n",
    ")\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(\"Train set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "# Save each split as a CSV file\n",
    "train_data.to_csv(\"train_labels_small.csv\", index=False)\n",
    "val_data.to_csv(\"val_labels_small.csv\", index=False)\n",
    "test_data.to_csv(\"test_labels_small.csv\", index=False)\n",
    "\n",
    "print(\"Train, validation, and test splits saved as CSV files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
