{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Script That Converts Image into a normalized grayscale tensor, with features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0118,  0.0144,  0.0092,  ..., -0.4353, -0.1163, -0.0588],\n",
       "         [ 0.0039, -0.0222, -0.0222,  ..., -0.5399, -0.2993, -0.0850],\n",
       "         [-0.0013, -0.0118, -0.0170,  ..., -0.6288, -0.4824, -0.0954],\n",
       "         ...,\n",
       "         [-0.6497, -0.7542, -0.7908,  ..., -0.7961, -0.7647, -0.7438],\n",
       "         [-0.7961, -0.7961, -0.8065,  ..., -0.7961, -0.7595, -0.7281],\n",
       "         [-0.8327, -0.7961, -0.8065,  ..., -0.8013, -0.7647, -0.7281]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load OpenCV DNN\n",
    "model_path = \"C:/Users/ahmad/Desktop/EngagementML/opencv-dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "config_path = \"C:/Users/ahmad/Desktop/EngagementML/opencv-dnn/deploy.prototxt\"\n",
    "\n",
    "\n",
    "# Define a PyTorch transform for grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to 1-channel grayscale\n",
    "    transforms.ToTensor(),                         # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.75], std=[0.75])   # Normalize: shift and scale pixel values\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_frame(image_path, face_net, target_size=(80, 80)):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Prepare the image for the DNN\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(300, 300),\n",
    "                                 mean=(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:  # Confidence threshold\n",
    "            # Get the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x1, y1) = box.astype(\"int\")\n",
    "\n",
    "            # Ensure the bounding box is within the image dimensions\n",
    "            x, y, x1, y1 = max(0, x), max(0, y), min(w, x1), min(h, y1)\n",
    "\n",
    "            # Crop the face\n",
    "            cropped_face = image[y:y1, x:x1]\n",
    "\n",
    "            resized_face = cv2.resize(cropped_face, target_size)\n",
    "\n",
    "            # Convert the resized face to a PIL image\n",
    "            pil_face = Image.fromarray(cv2.cvtColor(resized_face, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            transformed_face_tensor = transform(pil_face)\n",
    "\n",
    "            return transformed_face_tensor\n",
    "\n",
    "\n",
    "# Load the pre-trained DNN model\n",
    "face_net = cv2.dnn.readNetFromCaffe(config_path, model_path)\n",
    "\n",
    "\n",
    "img_path = r'C:\\Users\\ahmad\\Desktop\\EngagementML\\resized_faces\\51004210183.jpg_face0.jpg'\n",
    "\n",
    "preprocess_frame(img_path, face_net, target_size=(80, 80))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacesDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_csv, transform=None, num_frames=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing video frames organized in folders.\n",
    "            label_csv (str): Path to the CSV file containing ClipID and labels.\n",
    "            transform (callable, optional): Optional transformations to apply to frames.\n",
    "            num_frames (int): Number of frames per video to include as features.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = self.load_labels(label_csv)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "\n",
    "    def load_labels(self, label_csv):\n",
    "        \"\"\"\n",
    "        Load labels from the CSV file and return a dictionary of ClipID to label mappings.\n",
    "        \"\"\"\n",
    "        label_df = pd.read_csv(label_csv)\n",
    "        df_map = {}\n",
    "        for _, row in label_df.iterrows():\n",
    "            df_map[row['ClipID']] = row['Boredom']\n",
    "        return df_map\n",
    "\n",
    "    def get_file_paths(self, clip_id):\n",
    "        \"\"\"\n",
    "        Get sorted file paths for frames in a given clip folder.\n",
    "        \"\"\"\n",
    "        clip_folder = os.path.join(self.data_dir, f\"{clip_id}_frames\")\n",
    "        frame_files = sorted(os.listdir(clip_folder))\n",
    "        return [os.path.join(clip_folder, frame) for frame in frame_files[:self.num_frames]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample (frames and label) based on the index.\n",
    "        \"\"\"\n",
    "        clip_id = list(self.labels.keys())[idx]\n",
    "        label = self.labels[clip_id]\n",
    "\n",
    "        # Get file paths for the frames\n",
    "        frame_paths = self.get_file_paths(clip_id)\n",
    "\n",
    "        # Load and preprocess the frames\n",
    "        frames = []\n",
    "        for frame_path in frame_paths:\n",
    "            processed_frame = preprocess_frame(frame_path, self.face_net, target_size=self.target_size)\n",
    "            if processed_frame is not None:\n",
    "                frames.append(processed_frame)\n",
    "\n",
    "        # Stack all frames into a single tensor\n",
    "        frames_tensor = torch.stack(frames) if frames else torch.zeros(self.num_frames, 1, *self.target_size)\n",
    "\n",
    "        return frames_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
